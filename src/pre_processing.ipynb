{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "Text mining is used to obtain the _top k words_ appearing in the dataset that have been determined to be the most informative. These _top k words_ are used by the rule learner to generate rule sets utilized for predication. The text mining phase is conducted in the following order:\n",
    "\n",
    "#### Data Extraction\n",
    "Load all datasets to be fed into pre-processing\n",
    "\n",
    "#### 1. Pre-Processing\n",
    "* **1.1** Tokenization\n",
    "* **1.2** Stop word removal\n",
    "* **1.3** Stemming\n",
    "\n",
    "#### 2. Feature Selection\n",
    "* **2.1** Tf\\*Idf\n",
    "* **2.2** Info-Gain\n",
    "\n",
    "#### 3. Resulting Set\n",
    "\n",
    "| Id  | Word | Weight | Severity |\n",
    "| --- | ---- | ------ | -------- |\n",
    "|   1 | pred | 0.5551 |        3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jude/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jude/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets\n",
    "* load csv file\n",
    "* extract needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path, sep=',', encoding='ISO-8859-1')\n",
    "    raw_data = np.array(df)\n",
    "    \n",
    "    # get the columns for Subject and Severity Rating\n",
    "    extract_cols = [1, 2]\n",
    "    del_cols = np.delete(np.arange(raw_data.shape[1]), extract_cols)\n",
    "    data = np.delete(raw_data, del_cols, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Build 5.3: Unitialized Variables' 3]\n",
      " ['Build 5.3 FSW: Typecast Mismatch in Memory Deallocation' 3]\n",
      " ['Build 5.3 FSW: Parameter Type Mismatch' 3]\n",
      " ...\n",
      " ['The SIS and SDD are not listed within Table 1-9 of the SDP' 4]\n",
      " ['Incorrect and Incomplete traceability between the PSMP and the SDR' 5]\n",
      " ['Incorrect and Incomplete traceability between the PSMP and the SDR' 5]]\n"
     ]
    }
   ],
   "source": [
    "print(load_data('../dataset/raw/pitsA.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Processing\n",
    "\n",
    "## 1.1 Tokenization\n",
    "The steps outlined in the paper:\n",
    "1. All punctuation are replaced with spaces\n",
    "2. remove non-printable characters\n",
    "3. convert all letters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    # remove non-printable characters (ASCII ONLY)\n",
    "    \n",
    "    \n",
    "    # convert all letters to lowercase\n",
    "    lowercase_string = data.lower()\n",
    "    \n",
    "    # replace all punctuation with spaces\n",
    "    remove_punctuation = ''\n",
    "    for char in lowercase_string:\n",
    "        if char in punctuations:\n",
    "            remove_punctuation += ' '\n",
    "        else:\n",
    "            remove_punctuation += char\n",
    "    \n",
    "    # return word tokens\n",
    "    return nltk.word_tokenize(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['build', '5', '3', 'unitialized', 'variables']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize('Build 5.3: Unitialized Variables'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Stop Word Removal\n",
    "In the original paper, the number of stop words used is 262. The number of english stop words included in the nltk library is 179."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    \n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token not in eng_stopwords:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l3', 'sfs', '887', 'incomplete', 'traceability', 'l4', 'requirements']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(tokenize('L3-SFS-887 incomplete traceability to the L4 requirements')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Stemming\n",
    "Porter stemmer was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(no_stop_tokens):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    result = []\n",
    "    for token in no_stop_tokens:\n",
    "        result.append(porter_stemmer.stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l3', 'sf', '887', 'incomplet', 'traceabl', 'l4', 'requir']\n"
     ]
    }
   ],
   "source": [
    "print(stem_words(remove_stopwords(tokenize('L3-SFS-887 incomplete traceability to the L4 requirements'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tf\\*Idf\n",
    "The orginal paper has experimented with different values for the _top k_ words and decided on k=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider making top_k into a parameter\n",
    "def apply_tfidf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # get array of indices sorted by the tfidf\n",
    "    indices = np.argsort(vectorizer.idf_)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    \n",
    "    # create a new list of the features based on sorted indices \n",
    "    top_k = 100\n",
    "    top_features = [features[i] for i in indices[:top_k]]\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['requir', 'sr', 'test', 'l3', 'obc', 'engcntrl', 'sf', 'script', 'miss', 'build', 'code', 'rvm', 'l4', 'dh', 'inconsist', 'projecta', 'link', 'l2', 'ac', 'rqt', 'incomplet', 'trace', 'traceabl', 'sc', 'fsw', 'v1', 'issu', 'cm', 'fsrd', 'analysi', 'fpr', 'address', 'adequ', 'text', 'procedur', 'initi', 'verifi', 'vml', 'incorrect', 'softwar', 'rta', 'unclear', 'srup', 'satisfi', 'intent', 'uplink', 'prd', 'fulli', 'b5', 'mode', 'instanc', 'data', 'variabl', 'function', 'bound', 'control', 'command', 'defin', 'array', 'vm', 'flight', 'ta', 'symbol', 'use', 'possibl', 'pointer', 'exist', 'non', 'rm', 'associ', '11', 'fgi', 'v0', 'req', 'access', 'surom', 'case', 'unnecessari', 'fs', 'definit', 'default', 'line', 'may', 'attitud', 'cast', 'flow', '12', '04', 'unusu', 'within', 'return', 'switch', 'specif', 'memori', 'icd', 'referenc', 'fp', 'design', '10', 'macro']\n"
     ]
    }
   ],
   "source": [
    "#apply_tfidf(stem_words(remove_stopwords(tokenize('L3-SFS-887 incomplete traceability to the L4 requirements'))))\n",
    "\n",
    "# load dataset from csv\n",
    "pitsA = load_data('../dataset/raw/pitsA.csv')\n",
    "\n",
    "# tokenize, stem and concat to string for each summary\n",
    "documents = []\n",
    "for report in pitsA:\n",
    "    tokens = tokenize(report[0])\n",
    "    no_stopwords = remove_stopwords(tokens)\n",
    "    stemmed = stem_words(no_stopwords)\n",
    "    \n",
    "    seperator = ' '\n",
    "    documents.append(seperator.join(stemmed))\n",
    "\n",
    "# get top k=100 words based on tf*idf\n",
    "top_k = apply_tfidf(documents)\n",
    "print(top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Info-Gain\n",
    "Rerank _top k_ words.\n",
    "\n",
    "Defination of mutual information methods used from sklearn appear to be the same as info gain and has been used here.\n",
    "\n",
    "#### Method\n",
    "1. create feature matrix and target vector\n",
    "2. generate information gain weights for features in dataset\n",
    "3. map weight of features to list of top k words\n",
    "4. sort top k words by weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_top_k(documents, top_k):\n",
    "    # create feature matrix and target vector\n",
    "    # X = text subject\n",
    "    # y = subject severity\n",
    "    X, y = [], []\n",
    "    for document in documents:\n",
    "        X.append(document[0])\n",
    "        y.append(document[1])\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # store features with info gain weight\n",
    "    res = dict(zip(vectorizer.get_feature_names(),\n",
    "               mutual_info_classif(X_vec, y, discrete_features=True)\n",
    "               ))\n",
    "    #print(res)\n",
    "    \n",
    "    # map weights to top k words\n",
    "    weights = []\n",
    "    for k in top_k:\n",
    "        weights.append(res.get(k))\n",
    "    #print(weights)\n",
    "    \n",
    "    # sort by weight\n",
    "    indices = np.argsort(weights)\n",
    "    ranked_features = [top_k[i] for i in indices]\n",
    "    \n",
    "    return ranked_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default',\n",
      " 'switch',\n",
      " 'possibl',\n",
      " 'control',\n",
      " 'fp',\n",
      " 'design',\n",
      " 'flight',\n",
      " 'flow',\n",
      " 'may',\n",
      " 'case',\n",
      " 'attitud',\n",
      " 'mode',\n",
      " 'return',\n",
      " 'uplink',\n",
      " 'address',\n",
      " 'adequ',\n",
      " 'line',\n",
      " 'procedur',\n",
      " 'specif',\n",
      " '11',\n",
      " 'fpr',\n",
      " 'vm',\n",
      " 'initi',\n",
      " 'definit',\n",
      " 'associ',\n",
      " 'bound',\n",
      " 'fulli',\n",
      " 'memori',\n",
      " 'command',\n",
      " 'within',\n",
      " 'pointer',\n",
      " 'softwar',\n",
      " 'access',\n",
      " 'icd',\n",
      " 'b5',\n",
      " 'array',\n",
      " 'use',\n",
      " 'symbol',\n",
      " '12',\n",
      " '10',\n",
      " 'function',\n",
      " 'non',\n",
      " 'fgi',\n",
      " 'variabl',\n",
      " 'incorrect',\n",
      " 'ta',\n",
      " 'fs',\n",
      " 'inconsist',\n",
      " 'macro',\n",
      " 'unusu',\n",
      " 'defin',\n",
      " 'prd',\n",
      " 'data',\n",
      " 'cast',\n",
      " 'verifi',\n",
      " 'srup',\n",
      " 'v0',\n",
      " 'unclear',\n",
      " 'req',\n",
      " '04',\n",
      " 'unnecessari',\n",
      " 'referenc',\n",
      " 'intent',\n",
      " 'l2',\n",
      " 'exist',\n",
      " 'instanc',\n",
      " 'analysi',\n",
      " 'surom',\n",
      " 'vml',\n",
      " 'rm',\n",
      " 'satisfi',\n",
      " 'code',\n",
      " 'link',\n",
      " 'fsw',\n",
      " 'rta',\n",
      " 'build',\n",
      " 'incomplet',\n",
      " 'dh',\n",
      " 'v1',\n",
      " 'trace',\n",
      " 'obc',\n",
      " 'ac',\n",
      " 'projecta',\n",
      " 'fsrd',\n",
      " 'l4',\n",
      " 'text',\n",
      " 'traceabl',\n",
      " 'sc',\n",
      " 'sf',\n",
      " 'issu',\n",
      " 'cm',\n",
      " 'sr',\n",
      " 'l3',\n",
      " 'miss',\n",
      " 'rqt',\n",
      " 'requir',\n",
      " 'test',\n",
      " 'engcntrl',\n",
      " 'script',\n",
      " 'rvm']\n"
     ]
    }
   ],
   "source": [
    "#apply_tfidf(stem_words(remove_stopwords(tokenize('L3-SFS-887 incomplete traceability to the L4 requirements'))))\n",
    "\n",
    "# load dataset from csv\n",
    "pitsA = load_data('../dataset/raw/pitsA.csv')\n",
    "\n",
    "# tokenize, stem and concat to string for each summary\n",
    "documents = []\n",
    "for report in pitsA:\n",
    "    subject = report[0]\n",
    "    severity = report[1]\n",
    "    \n",
    "    tokens = tokenize(report[0])\n",
    "    no_stopwords = remove_stopwords(tokens)\n",
    "    stemmed = stem_words(no_stopwords)\n",
    "    \n",
    "    seperator = ' '\n",
    "    document = [seperator.join(stemmed), severity]\n",
    "    documents.append(document)\n",
    "\n",
    "# get top k=100 words based on tf*idf\n",
    "subjects = []\n",
    "for report in documents:\n",
    "    subjects.append(report[0])\n",
    "top_k = apply_tfidf(subjects)\n",
    "#print(top_k)\n",
    "\n",
    "# reorder top k words with info-gain\n",
    "ranked_top_k = rank_top_k(documents, top_k)\n",
    "\n",
    "# print ranked top k words\n",
    "pprint(ranked_top_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
